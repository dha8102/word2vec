{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Basic word2vec example.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import time\n",
    "toc = time.clock()\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 17005207\n",
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5235, 3084, 12, 6, 195, 2, 3135, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "3084 originated -> 12 as\n",
      "3084 originated -> 5235 anarchism\n",
      "12 as -> 3084 originated\n",
      "12 as -> 6 a\n",
      "6 a -> 195 term\n",
      "6 a -> 12 as\n",
      "195 term -> 6 a\n",
      "195 term -> 2 of\n",
      "Initialized\n",
      "Average loss at step  0 :  268.0386657714844\n",
      "Nearest to at: walton, nacional, satirically, aspect, stanzas, outside, bytecode, indentation,\n",
      "Nearest to many: derails, bradford, oberlin, misandry, dogg, equipping, scottie, qazaqstan,\n",
      "Nearest to united: guilds, pays, reunited, noodles, berets, lysosomes, eliminated, thanksgiving,\n",
      "Nearest to this: flies, shuster, marduk, consumers, negotiators, canonicity, sachs, garret,\n",
      "Nearest to more: fists, charenton, berries, result, glider, gesellschaft, waste, myrdal,\n",
      "Nearest to who: intellectualism, mulder, mimicking, gev, intergovernmental, ghanaian, parachutes, descent,\n",
      "Nearest to seven: resin, police, abrahamic, vue, ethene, hannes, therapy, basket,\n",
      "Nearest to if: crewmembers, cookbooks, merseyside, remark, gsm, aes, cutters, complement,\n",
      "Nearest to from: inexplicable, commitment, secondo, seanad, austrian, synthase, ethene, consecrated,\n",
      "Nearest to would: fuelled, thc, cyborgs, paragon, rockwell, accent, kovacs, mst,\n",
      "Nearest to used: acadian, sliding, deformities, aired, sabha, gottes, paladin, jura,\n",
      "Nearest to however: weighing, tenets, finnegans, archaeopteryx, metropolises, amphetamine, olympians, preside,\n",
      "Nearest to these: syndicalism, applying, sentries, alleviated, protections, safin, calming, coslet,\n",
      "Nearest to may: piotr, diesels, envelopes, muppet, bullying, hue, malm, intermediates,\n",
      "Nearest to s: pauline, ticonderoga, ably, nerd, recreationally, dhimmis, vega, maldives,\n",
      "Nearest to for: barnabas, disagree, goodall, newborns, review, organelle, cebu, traynor,\n",
      "Average loss at step  2000 :  114.41637878799439\n",
      "Average loss at step  4000 :  52.640378737449645\n",
      "Average loss at step  6000 :  32.62648760294914\n",
      "Average loss at step  8000 :  24.03075248479843\n",
      "Average loss at step  10000 :  17.582113122820854\n",
      "Nearest to at: lymphoma, in, and, allen, one, asparagales, borman, of,\n",
      "Nearest to many: lymphoma, the, prayer, assistive, thousands, scottie, mammalia, hound,\n",
      "Nearest to united: lymphoma, vedic, eliminated, agave, pc, holy, inventor, mistakes,\n",
      "Nearest to this: the, his, intercourse, one, bob, bar, a, also,\n",
      "Nearest to more: worse, result, waste, sieges, regard, hammond, minister, monsters,\n",
      "Nearest to who: and, algae, aetius, irrigation, given, century, pus, austin,\n",
      "Nearest to seven: zero, nine, gland, eight, four, agave, six, police,\n",
      "Nearest to if: and, remark, minutes, or, man, almost, initial, jpg,\n",
      "Nearest to from: in, for, commitment, austrian, of, and, prosecution, backwards,\n",
      "Nearest to would: fuelled, that, mathbf, altenberg, to, crafts, kournikova, gb,\n",
      "Nearest to used: spawned, sliding, merchant, mathbf, investment, deke, aired, closely,\n",
      "Nearest to however: agave, alleged, phi, statistics, similar, archie, humorous, defensive,\n",
      "Nearest to these: safin, muhammad, biscuit, loss, archie, yeast, christians, pseudocode,\n",
      "Nearest to may: to, nine, lived, carrel, burr, agreed, critical, bertram,\n",
      "Nearest to s: and, lymphoma, of, ensuring, the, yards, italian, dynamic,\n",
      "Nearest to for: and, in, with, of, from, samplers, serious, disagree,\n",
      "Average loss at step  12000 :  14.050544499754906\n",
      "Average loss at step  14000 :  11.605387392997741\n",
      "Average loss at step  16000 :  9.83943310880661\n",
      "Average loss at step  18000 :  8.362593646228314\n",
      "Average loss at step  20000 :  8.015551531791687\n",
      "Nearest to at: in, and, lymphoma, on, of, under, circ, from,\n",
      "Nearest to many: their, operatorname, prayer, lymphoma, thousands, assistive, scottie, mechanically,\n",
      "Nearest to united: lymphoma, ell, vedic, eliminated, pays, operatorname, pc, holy,\n",
      "Nearest to this: the, it, his, a, which, one, intercourse, scope,\n",
      "Nearest to more: worse, severus, waste, hammond, result, sieges, regard, honorius,\n",
      "Nearest to who: and, also, attach, backs, algae, aetius, given, he,\n",
      "Nearest to seven: nine, zero, eight, four, five, six, two, one,\n",
      "Nearest to if: and, or, aes, remark, minutes, as, circ, is,\n",
      "Nearest to from: in, for, and, of, at, austrian, commitment, nine,\n",
      "Nearest to would: to, can, may, altenberg, circ, ass, fuelled, must,\n",
      "Nearest to used: spawned, sliding, gollancz, aired, merchant, dasyprocta, andamanese, circ,\n",
      "Nearest to however: weighing, and, agave, alleged, baroque, officeholders, sirius, statistics,\n",
      "Nearest to these: the, muhammad, they, biscuit, safin, loss, pseudocode, intercourse,\n",
      "Nearest to may: to, would, can, nine, dasyprocta, lived, shanghai, should,\n",
      "Nearest to s: lymphoma, and, his, the, of, zero, or, yards,\n",
      "Nearest to for: and, of, in, with, from, to, on, or,\n",
      "Average loss at step  22000 :  7.0691179673671725\n",
      "Average loss at step  24000 :  6.862181081652642\n",
      "Average loss at step  26000 :  6.694680979728699\n",
      "Average loss at step  28000 :  6.365140948772431\n",
      "Average loss at step  30000 :  5.967813371896744\n",
      "Nearest to at: in, and, on, lymphoma, under, from, with, for,\n",
      "Nearest to many: the, abet, their, lymphoma, operatorname, prayer, studd, mechanically,\n",
      "Nearest to united: lymphoma, pays, eliminated, ell, vedic, reunited, pc, holy,\n",
      "Nearest to this: the, it, which, a, his, one, intercourse, its,\n",
      "Nearest to more: worse, severus, sieges, waste, bronze, regard, retraction, hammond,\n",
      "Nearest to who: and, also, he, backs, attach, not, they, given,\n",
      "Nearest to seven: nine, eight, four, five, six, zero, three, two,\n",
      "Nearest to if: or, and, aes, remark, when, lag, but, is,\n",
      "Nearest to from: in, for, and, at, with, by, of, into,\n",
      "Nearest to would: to, can, may, must, altenberg, ass, will, kournikova,\n",
      "Nearest to used: spawned, gollancz, merchant, sliding, aired, dasyprocta, circ, andamanese,\n",
      "Nearest to however: and, amphetamine, weighing, alleged, agave, or, officeholders, baroque,\n",
      "Nearest to these: the, they, biscuit, pseudocode, cultivating, intercourse, circ, safin,\n",
      "Nearest to may: would, can, to, should, will, dasyprocta, nine, hue,\n",
      "Nearest to s: and, lymphoma, his, two, zero, abet, the, or,\n",
      "Nearest to for: and, in, with, from, of, or, to, by,\n",
      "Average loss at step  32000 :  5.9787898914814\n",
      "Average loss at step  34000 :  5.721377164244652\n",
      "Average loss at step  36000 :  5.789257310032845\n",
      "Average loss at step  38000 :  5.494563570618629\n",
      "Average loss at step  40000 :  5.231253429174423\n",
      "Nearest to at: in, on, under, lymphoma, from, albury, and, with,\n",
      "Nearest to many: some, their, abet, the, these, studd, several, operatorname,\n",
      "Nearest to united: lymphoma, pays, eliminated, ell, vedic, operatorname, holy, reunited,\n",
      "Nearest to this: which, it, the, that, a, one, his, homomorphism,\n",
      "Nearest to more: worse, severus, sieges, gesellschaft, bronze, hammond, oceans, monsters,\n",
      "Nearest to who: and, he, also, backs, they, intellectualism, which, not,\n",
      "Nearest to seven: eight, four, six, zero, five, nine, three, two,\n",
      "Nearest to if: or, aes, when, but, lag, remark, and, gsm,\n",
      "Nearest to from: in, at, into, akita, for, of, abet, and,\n",
      "Nearest to would: may, can, to, must, will, altenberg, could, vdc,\n",
      "Nearest to used: spawned, gollancz, sliding, merchant, aired, circ, dasyprocta, jura,\n",
      "Nearest to however: archaeopteryx, but, weighing, agave, amphetamine, alleged, martian, officeholders,\n",
      "Nearest to these: they, biscuit, three, pseudocode, many, the, igg, cultivating,\n",
      "Nearest to may: can, would, should, will, goo, to, transonic, could,\n",
      "Nearest to s: lymphoma, his, zero, abet, two, and, girardeau, the,\n",
      "Nearest to for: or, and, of, with, from, to, in, abet,\n",
      "Average loss at step  42000 :  5.320757427096367\n",
      "Average loss at step  44000 :  5.205952777504921\n",
      "Average loss at step  46000 :  5.228181514143944\n",
      "Average loss at step  48000 :  5.235496768832207\n",
      "Average loss at step  50000 :  5.0158892529010775\n",
      "Nearest to at: in, on, johansen, under, lymphoma, from, during, albury,\n",
      "Nearest to many: some, several, these, batcave, scholars, abet, studd, operatorname,\n",
      "Nearest to united: lymphoma, lysosomes, pays, eliminated, ell, hobby, vedic, reunited,\n",
      "Nearest to this: which, it, the, that, one, a, mishnayot, homomorphism,\n",
      "Nearest to more: worse, severus, most, sieges, gesellschaft, oceans, nash, hammond,\n",
      "Nearest to who: and, he, also, they, backs, not, which, still,\n",
      "Nearest to seven: eight, six, five, four, three, nine, zero, two,\n",
      "Nearest to if: when, and, or, but, aes, lag, johansen, for,\n",
      "Nearest to from: in, into, kapoor, at, for, of, and, akita,\n",
      "Nearest to would: can, may, must, to, will, could, altenberg, might,\n",
      "Nearest to used: spawned, sliding, gollancz, aired, merchant, batcave, intermolecular, jura,\n",
      "Nearest to however: but, archaeopteryx, and, weighing, agave, amphetamine, officeholders, martian,\n",
      "Nearest to these: they, many, kapoor, the, biscuit, igg, pseudocode, some,\n",
      "Nearest to may: can, would, will, should, to, goo, could, must,\n",
      "Nearest to s: lymphoma, his, and, abet, of, five, zero, four,\n",
      "Nearest to for: or, of, in, with, and, kapoor, abet, to,\n",
      "Average loss at step  52000 :  5.04770210802555\n",
      "Average loss at step  54000 :  5.187874938130379\n",
      "Average loss at step  56000 :  5.057319740056991\n",
      "Average loss at step  58000 :  5.069425178647041\n",
      "Average loss at step  60000 :  4.949735365390778\n",
      "Nearest to at: in, on, under, from, lymphoma, johansen, during, arin,\n",
      "Nearest to many: some, several, these, batcave, scholars, various, studd, two,\n",
      "Nearest to united: lymphoma, pays, lysosomes, eliminated, hobby, stratofortress, operatorname, ell,\n",
      "Nearest to this: which, it, that, the, one, ursus, a, albury,\n",
      "Nearest to more: worse, most, severus, oceans, less, nash, gesellschaft, graces,\n",
      "Nearest to who: he, they, which, also, backs, still, and, not,\n",
      "Nearest to seven: eight, six, nine, five, four, three, zero, operatorname,\n",
      "Nearest to if: when, but, or, aes, lag, is, johansen, circ,\n",
      "Nearest to from: in, into, at, kapoor, during, akita, abet, of,\n",
      "Nearest to would: can, may, must, will, to, could, might, michelob,\n",
      "Nearest to used: sliding, spawned, gollancz, aired, intermolecular, held, batcave, contraception,\n",
      "Nearest to however: but, archaeopteryx, weighing, agave, martian, which, and, officeholders,\n",
      "Nearest to these: many, they, some, kapoor, two, such, biscuit, three,\n",
      "Nearest to may: can, would, should, will, sutter, could, might, must,\n",
      "Nearest to s: lymphoma, zero, abet, ursus, his, and, five, miami,\n",
      "Nearest to for: or, of, in, kapoor, and, abet, after, with,\n",
      "Average loss at step  62000 :  5.03007860481739\n",
      "Average loss at step  64000 :  4.8357715483903885\n",
      "Average loss at step  66000 :  4.6108553872108455\n",
      "Average loss at step  68000 :  4.9765945707559585\n",
      "Average loss at step  70000 :  4.892771905899048\n",
      "Nearest to at: in, under, during, on, lymphoma, johansen, arin, from,\n",
      "Nearest to many: some, several, these, various, scholars, mitral, batcave, other,\n",
      "Nearest to united: lymphoma, pays, lysosomes, hobby, microcebus, stratofortress, operatorname, eliminated,\n",
      "Nearest to this: which, it, the, that, ursus, one, microcebus, callithrix,\n",
      "Nearest to more: worse, most, less, thaler, severus, higher, verge, desiring,\n",
      "Nearest to who: he, they, which, also, backs, still, often, and,\n",
      "Nearest to seven: eight, six, five, four, nine, three, zero, pulau,\n",
      "Nearest to if: when, but, lag, aes, or, for, johansen, where,\n",
      "Nearest to from: into, in, during, kapoor, at, akita, and, circ,\n",
      "Nearest to would: can, may, will, must, could, might, to, michelob,\n",
      "Nearest to used: sliding, spawned, gollancz, held, contraception, aired, batcave, circ,\n",
      "Nearest to however: but, and, archaeopteryx, agave, including, martian, while, amphetamine,\n",
      "Nearest to these: they, many, some, such, several, kapoor, those, biscuit,\n",
      "Nearest to may: can, would, will, should, could, might, must, sutter,\n",
      "Nearest to s: lymphoma, his, miami, abet, ursus, zero, nw, and,\n",
      "Nearest to for: or, of, in, and, callithrix, kapoor, abet, johansen,\n",
      "Average loss at step  72000 :  4.75644814324379\n",
      "Average loss at step  74000 :  4.812922763109207\n",
      "Average loss at step  76000 :  4.726623168110847\n",
      "Average loss at step  78000 :  4.800377856850624\n",
      "Average loss at step  80000 :  4.79123249399662\n",
      "Nearest to at: in, under, during, on, from, johansen, lymphoma, arin,\n",
      "Nearest to many: some, several, these, other, scholars, mitral, various, such,\n",
      "Nearest to united: pays, lymphoma, hobby, stratofortress, lysosomes, traveler, holy, reunited,\n",
      "Nearest to this: which, it, that, the, ursus, microcebus, some, callithrix,\n",
      "Nearest to more: most, worse, less, very, thaler, severus, verge, higher,\n",
      "Nearest to who: he, they, often, and, still, backs, which, she,\n",
      "Nearest to seven: eight, six, five, four, nine, three, one, zero,\n",
      "Nearest to if: when, but, lag, aes, where, johansen, or, circ,\n",
      "Nearest to from: into, in, during, at, of, through, nine, akita,\n",
      "Nearest to would: may, can, will, must, could, might, to, michelob,\n",
      "Nearest to used: sliding, spawned, contraception, known, gollancz, held, considered, aired,\n",
      "Nearest to however: but, when, archaeopteryx, that, and, while, including, agave,\n",
      "Nearest to these: many, some, such, they, those, several, all, igg,\n",
      "Nearest to may: can, would, will, should, could, might, must, sutter,\n",
      "Nearest to s: his, zero, lymphoma, ursus, abet, miami, nw, lunch,\n",
      "Nearest to for: kapoor, or, of, callithrix, in, abet, and, operatorname,\n",
      "Average loss at step  82000 :  4.761127617955208\n",
      "Average loss at step  84000 :  4.7430412555933\n",
      "Average loss at step  86000 :  4.769232717752456\n",
      "Average loss at step  88000 :  4.757241212129593\n",
      "Average loss at step  90000 :  4.726332277536392\n",
      "Nearest to at: in, under, during, on, from, johansen, without, after,\n",
      "Nearest to many: some, several, these, various, mitral, other, scholars, such,\n",
      "Nearest to united: lymphoma, pays, hobby, stratofortress, lysosomes, traveler, microcebus, operatorname,\n",
      "Nearest to this: which, it, the, that, ursus, a, some, callithrix,\n",
      "Nearest to more: less, most, worse, thaler, very, verge, severus, higher,\n",
      "Nearest to who: he, and, they, often, which, still, she, backs,\n",
      "Nearest to seven: eight, five, six, four, nine, three, zero, one,\n",
      "Nearest to if: when, but, where, lag, aes, johansen, though, circ,\n",
      "Nearest to from: in, into, during, at, akita, through, kapoor, circ,\n",
      "Nearest to would: may, can, will, could, must, might, to, michelob,\n",
      "Nearest to used: known, spawned, gollancz, held, sliding, considered, available, found,\n",
      "Nearest to however: but, peacocks, while, and, when, that, including, archaeopteryx,\n",
      "Nearest to these: many, some, such, they, those, several, the, all,\n",
      "Nearest to may: can, would, will, should, might, could, must, sutter,\n",
      "Nearest to s: and, abet, ursus, his, lymphoma, zero, nw, lunch,\n",
      "Nearest to for: kapoor, or, candide, abet, callithrix, in, symmetries, of,\n",
      "Average loss at step  92000 :  4.654796751260758\n",
      "Average loss at step  94000 :  4.713895084857941\n",
      "Average loss at step  96000 :  4.690544680953026\n",
      "Average loss at step  98000 :  4.594156632065773\n",
      "Average loss at step  100000 :  4.685896288394928\n",
      "Nearest to at: in, under, during, after, from, arin, lymphoma, on,\n",
      "Nearest to many: some, several, these, various, mitral, scholars, such, all,\n",
      "Nearest to united: pays, hobby, stratofortress, lymphoma, traveler, lysosomes, reunited, holy,\n",
      "Nearest to this: which, it, the, that, some, ursus, callithrix, there,\n",
      "Nearest to more: less, most, worse, very, verge, thaler, severus, higher,\n",
      "Nearest to who: he, she, often, still, they, also, backs, which,\n",
      "Nearest to seven: eight, six, five, four, nine, three, zero, two,\n",
      "Nearest to if: when, but, where, lag, stenella, though, circ, is,\n",
      "Nearest to from: in, into, during, at, through, kapoor, akita, within,\n",
      "Nearest to would: may, can, will, could, must, might, to, should,\n",
      "Nearest to used: known, considered, held, found, available, gollancz, spawned, sliding,\n",
      "Nearest to however: but, and, peacocks, when, that, while, including, which,\n",
      "Nearest to these: many, some, such, several, they, those, which, both,\n",
      "Nearest to may: can, would, will, might, could, should, must, sutter,\n",
      "Nearest to s: lymphoma, abet, ursus, four, his, lunch, girardeau, zero,\n",
      "Nearest to for: kapoor, callithrix, candide, of, abet, or, and, without,\n",
      "Please install sklearn, matplotlib, and scipy to show embeddings.\n",
      "No module named 'matplotlib'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "\n",
    "def word2vec_basic(log_dir):\n",
    "  \"\"\"Example of building, training and visualizing a word2vec model.\"\"\"\n",
    "  # Create the directory for TensorBoard variables if there is not.\n",
    "  if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "  # Step 1: Download the data.\n",
    "  url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "  # pylint: disable=redefined-outer-name\n",
    "  def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    local_filename = os.path.join(gettempdir(), filename)\n",
    "    if not os.path.exists(local_filename):\n",
    "      local_filename, _ = urllib.request.urlretrieve(url + filename,\n",
    "                                                     local_filename)\n",
    "    statinfo = os.stat(local_filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "      print('Found and verified', filename)\n",
    "    else:\n",
    "      print(statinfo.st_size)\n",
    "      raise Exception('Failed to verify ' + local_filename +\n",
    "                      '. Can you get to it with a browser?')\n",
    "    return local_filename\n",
    "\n",
    "  filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "  # Read the data into a list of strings.\n",
    "  def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "      data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "  vocabulary = read_data(filename)\n",
    "  print('Data size', len(vocabulary))\n",
    "\n",
    "  # Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "  vocabulary_size = 50000\n",
    "\n",
    "  def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "      dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "      index = dictionary.get(word, 0)\n",
    "      if index == 0:  # dictionary['UNK']\n",
    "        unk_count += 1\n",
    "      data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "  # Filling 4 global variables:\n",
    "  # data - list of codes (integers from 0 to vocabulary_size-1).\n",
    "  #   This is the original text but words are replaced by their codes\n",
    "  # count - map of words(strings) to count of occurrences\n",
    "  # dictionary - map of words(strings) to their codes(integers)\n",
    "  # reverse_dictionary - maps codes(integers) to words(strings)\n",
    "  data, count, unused_dictionary, reverse_dictionary = build_dataset(\n",
    "      vocabulary, vocabulary_size)\n",
    "  del vocabulary  # Hint to reduce memory.\n",
    "  print('Most common words (+UNK)', count[:5])\n",
    "  print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "  # Step 3: Function to generate a training batch for the skip-gram model.\n",
    "  def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builtin\n",
    "    if data_index + span > len(data):\n",
    "      data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "      context_words = [w for w in range(span) if w != skip_window]\n",
    "      words_to_use = random.sample(context_words, num_skips)\n",
    "      for j, context_word in enumerate(words_to_use):\n",
    "        batch[i * num_skips + j] = buffer[skip_window]\n",
    "        labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "      if data_index == len(data):\n",
    "        buffer.extend(data[0:span])\n",
    "        data_index = span\n",
    "      else:\n",
    "        buffer.append(data[data_index])\n",
    "        data_index += 1\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "  batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "  for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n",
    "          reverse_dictionary[labels[i, 0]])\n",
    "\n",
    "  # Step 4: Build and train a skip-gram model.\n",
    "\n",
    "  batch_size = 128\n",
    "  embedding_size = 128  # Dimension of the embedding vector.\n",
    "  skip_window = 1  # How many words to consider left and right.\n",
    "  num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "  num_sampled = 64  # Number of negative examples to sample.\n",
    "\n",
    "  # We pick a random validation set to sample nearest neighbors. Here we limit\n",
    "  # the validation samples to the words that have a low numeric ID, which by\n",
    "  # construction are also the most frequent. These 3 variables are used only for\n",
    "  # displaying model accuracy, they don't affect calculation.\n",
    "  valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "  valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "  valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "  graph = tf.Graph()\n",
    "\n",
    "  with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    with tf.name_scope('inputs'):\n",
    "      train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "      train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "      valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "      # Look up embeddings for inputs.\n",
    "      with tf.name_scope('embeddings'):\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "      # Construct the variables for the NCE loss\n",
    "      with tf.name_scope('weights'):\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "      with tf.name_scope('biases'):\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    with tf.name_scope('loss'):\n",
    "      loss = tf.reduce_mean(\n",
    "          tf.nn.nce_loss(\n",
    "              weights=nce_weights,\n",
    "              biases=nce_biases,\n",
    "              labels=train_labels,\n",
    "              inputs=embed,\n",
    "              num_sampled=num_sampled,\n",
    "              num_classes=vocabulary_size))\n",
    "\n",
    "    # Add the loss value as a scalar to summary.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    with tf.name_scope('optimizer'):\n",
    "      optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all\n",
    "    # embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "                                              valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Merge all summaries.\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "  # Step 5: Begin training.\n",
    "  num_steps = 100001\n",
    "\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    # Open a writer to write summaries.\n",
    "    writer = tf.summary.FileWriter(log_dir, session.graph)\n",
    "\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "      batch_inputs, batch_labels = generate_batch(batch_size, num_skips,\n",
    "                                                  skip_window)\n",
    "      feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "      # Define metadata variable.\n",
    "      run_metadata = tf.RunMetadata()\n",
    "\n",
    "      # We perform one update step by evaluating the optimizer op (including it\n",
    "      # in the list of returned values for session.run()\n",
    "      # Also, evaluate the merged op to get all summaries from the returned\n",
    "      # \"summary\" variable. Feed metadata variable to session for visualizing\n",
    "      # the graph in TensorBoard.\n",
    "      _, summary, loss_val = session.run([optimizer, merged, loss],\n",
    "                                         feed_dict=feed_dict,\n",
    "                                         run_metadata=run_metadata)\n",
    "      average_loss += loss_val\n",
    "\n",
    "      # Add returned summaries to writer in each step.\n",
    "      writer.add_summary(summary, step)\n",
    "      # Add metadata to visualize the graph for the last run.\n",
    "      if step == (num_steps - 1):\n",
    "        writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "      if step % 2000 == 0:\n",
    "        if step > 0:\n",
    "          average_loss /= 2000\n",
    "        # The average loss is an estimate of the loss over the last 2000\n",
    "        # batches.\n",
    "        print('Average loss at step ', step, ': ', average_loss)\n",
    "        average_loss = 0\n",
    "\n",
    "      # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "      if step % 10000 == 0:\n",
    "        sim = similarity.eval()\n",
    "        for i in xrange(valid_size):\n",
    "          valid_word = reverse_dictionary[valid_examples[i]]\n",
    "          top_k = 8  # number of nearest neighbors\n",
    "          nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "          log_str = 'Nearest to %s:' % valid_word\n",
    "          for k in xrange(top_k):\n",
    "            close_word = reverse_dictionary[nearest[k]]\n",
    "            log_str = '%s %s,' % (log_str, close_word)\n",
    "          print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    # Write corresponding labels for the embeddings.\n",
    "    with open(log_dir + '/metadata.tsv', 'w') as f:\n",
    "      for i in xrange(vocabulary_size):\n",
    "        f.write(reverse_dictionary[i] + '\\n')\n",
    "\n",
    "    # Save the model for checkpoints.\n",
    "    saver.save(session, os.path.join(log_dir, 'model.ckpt'))\n",
    "\n",
    "    # Create a configuration for visualizing embeddings with the labels in\n",
    "    # TensorBoard.\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = embeddings.name\n",
    "    embedding_conf.metadata_path = os.path.join(log_dir, 'metadata.tsv')\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "\n",
    "  writer.close()\n",
    "\n",
    "  # Step 6: Visualize the embeddings.\n",
    "\n",
    "  # pylint: disable=missing-docstring\n",
    "  # Function to draw visualization of distance between embeddings.\n",
    "  def plot_with_labels(low_dim_embs, labels, filename):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "      x, y = low_dim_embs[i, :]\n",
    "      plt.scatter(x, y)\n",
    "      plt.annotate(\n",
    "          label,\n",
    "          xy=(x, y),\n",
    "          xytext=(5, 2),\n",
    "          textcoords='offset points',\n",
    "          ha='right',\n",
    "          va='bottom')\n",
    "\n",
    "    plt.savefig(filename)\n",
    "\n",
    "  try:\n",
    "    # pylint: disable=g-import-not-at-top\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    tsne = TSNE(\n",
    "        perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "    plot_only = 500\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "    plot_with_labels(low_dim_embs, labels, os.path.join(gettempdir(),\n",
    "                                                        'tsne.png'))\n",
    "\n",
    "  except ImportError as ex:\n",
    "    print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "    print(ex)\n",
    "\n",
    "\n",
    "# All functionality is run after tf.app.run() (b/122547914). This could be split\n",
    "# up but the methods are laid sequentially with their usage for clarity.\n",
    "def main(unused_argv):\n",
    "  # Give a folder path as an argument with '--log_dir' to save\n",
    "  # TensorBoard summaries. Default is a log folder in current directory.\n",
    "  current_path = os.path.dirname(os.path.realpath(sys.argv[0]))\n",
    "\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--log_dir',\n",
    "      type=str,\n",
    "      default=os.path.join(current_path, 'log'),\n",
    "      help='The log directory for TensorBoard summaries.')\n",
    "  flags, unused_flags = parser.parse_known_args()\n",
    "  word2vec_basic(flags.log_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
