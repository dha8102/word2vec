{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Basic word2vec example.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import time\n",
    "toc = time.clock()\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 17005207\n",
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5244, 3084, 12, 6, 195, 2, 3136, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "3084 originated -> 5244 anarchism\n",
      "3084 originated -> 12 as\n",
      "12 as -> 3084 originated\n",
      "12 as -> 6 a\n",
      "6 a -> 12 as\n",
      "6 a -> 195 term\n",
      "195 term -> 6 a\n",
      "195 term -> 2 of\n",
      "Initialized\n",
      "Average loss at step  0 :  239.53187561035156\n",
      "Nearest to called: differentials, trickle, selected, infielders, fremen, dragonball, assemblages, ichij,\n",
      "Nearest to years: dekker, speleology, helpful, performs, afternoons, inhibit, irons, hashshashin,\n",
      "Nearest to about: blondie, gobind, everest, atlases, positively, kinsella, unbundling, qxd,\n",
      "Nearest to his: differing, twists, microwaves, wis, twofold, basquiat, colons, increment,\n",
      "Nearest to into: improve, lovejoy, lollards, diacritic, mirabeau, psychiatric, lc, lean,\n",
      "Nearest to system: jet, babylonians, ulyanov, entrenched, bl, guanosine, motivation, obtaining,\n",
      "Nearest to will: jawaharlal, eglise, riordan, proceeded, entire, mesures, detonates, sl,\n",
      "Nearest to to: ville, zines, oral, edict, statically, profile, limerick, designator,\n",
      "Nearest to all: adherence, abortive, litas, cardinality, flaw, cantonal, girlfriends, symbolize,\n",
      "Nearest to for: anteaters, dantzig, briefly, kamchatka, harassment, hertogenbosch, cinq, kildall,\n",
      "Nearest to seven: interconnected, feuerbach, deploying, powerplant, attracting, lived, edna, stoke,\n",
      "Nearest to no: transcontinental, mousa, ichthyology, saguinus, toned, infectors, ascension, mirrors,\n",
      "Nearest to over: teng, metropolitan, vh, periodicals, apollodorus, heathen, swayed, solicitor,\n",
      "Nearest to had: scarlatti, levee, hermes, themed, payments, stability, vatican, christiane,\n",
      "Nearest to when: overruns, mathfrak, fieldwork, decreasing, incidental, sprite, gchq, risc,\n",
      "Nearest to most: bower, shipowner, weaver, tagalog, reflector, straight, iris, durrell,\n",
      "Average loss at step  2000 :  114.55594264888764\n",
      "Average loss at step  4000 :  52.5022247235775\n",
      "Average loss at step  6000 :  33.05206275653839\n",
      "Average loss at step  8000 :  23.305913872480392\n",
      "Average loss at step  10000 :  17.811982341051102\n",
      "Nearest to called: selected, and, creature, environment, built, humans, rudolph, kinds,\n",
      "Nearest to years: mya, one, achieve, sister, helpful, released, dug, gimme,\n",
      "Nearest to about: blondie, qxd, processors, formerly, vs, allegiance, atlases, mathbf,\n",
      "Nearest to his: the, judgment, a, addict, completed, differing, organs, agave,\n",
      "Nearest to into: improve, diacritic, following, codes, austin, psychiatric, browns, saul,\n",
      "Nearest to system: jet, rudolph, babylonians, obtaining, implicit, match, stevens, motivation,\n",
      "Nearest to will: royal, entire, proceeded, although, allowed, wearing, singer, pleas,\n",
      "Nearest to to: and, in, organs, of, by, protocol, hit, dressed,\n",
      "Nearest to all: cardinality, adherence, individual, allen, consequent, examination, abortive, vowels,\n",
      "Nearest to for: and, in, of, with, fetus, intentions, passenger, alexandria,\n",
      "Nearest to seven: nine, eight, three, five, zero, archie, dim, remixes,\n",
      "Nearest to no: transcontinental, shrugged, mirrors, suits, historically, essays, calls, designed,\n",
      "Nearest to over: agave, metropolitan, loss, finally, devices, rz, operate, advocated,\n",
      "Nearest to had: hermes, finally, hermann, ep, themed, occurs, onset, hogeschool,\n",
      "Nearest to when: qualities, claim, in, decreasing, hakama, mathfrak, dead, dispute,\n",
      "Nearest to most: reflector, tagalog, pure, straight, finally, gb, train, safe,\n",
      "Average loss at step  12000 :  14.060392154693604\n",
      "Average loss at step  14000 :  11.994295351147652\n",
      "Average loss at step  16000 :  9.83705069732666\n",
      "Average loss at step  18000 :  8.606603821039199\n",
      "Average loss at step  20000 :  8.073829251766204\n",
      "Nearest to called: and, selected, epistle, creature, built, brew, dasyprocta, rudolph,\n",
      "Nearest to years: mya, melody, two, dug, achieve, released, helpful, inhibit,\n",
      "Nearest to about: qxd, blondie, nine, three, five, processors, seven, circ,\n",
      "Nearest to his: the, their, s, judgment, operatorname, this, its, a,\n",
      "Nearest to into: following, of, on, codes, diacritic, improve, and, depicted,\n",
      "Nearest to system: ulyanov, jet, babylonians, rudolph, match, obtaining, implicit, priestly,\n",
      "Nearest to will: would, is, regulatory, was, although, wearing, proceeded, mesures,\n",
      "Nearest to to: and, imran, nine, apatosaurus, for, organs, dressed, in,\n",
      "Nearest to all: cardinality, sayings, adherence, individual, abortive, dunstan, many, allen,\n",
      "Nearest to for: in, and, of, with, by, from, to, as,\n",
      "Nearest to seven: nine, eight, five, three, zero, six, four, dasyprocta,\n",
      "Nearest to no: suits, shrugged, transcontinental, marcellus, mirrors, historically, apatosaurus, he,\n",
      "Nearest to over: agave, metropolitan, agouti, periodicals, berg, with, heathen, recorder,\n",
      "Nearest to had: has, have, hermes, finally, was, by, ep, agouti,\n",
      "Nearest to when: and, qualities, claim, incidental, agouti, operatorname, gothic, in,\n",
      "Nearest to most: reflector, weaver, tagalog, finally, brutality, pure, straight, gb,\n",
      "Average loss at step  22000 :  6.873730712413788\n",
      "Average loss at step  24000 :  6.924630394220352\n",
      "Average loss at step  26000 :  6.687309416890145\n",
      "Average loss at step  28000 :  6.376100091338158\n",
      "Average loss at step  30000 :  5.843454604148865\n",
      "Nearest to called: and, epistle, brew, selected, birkenau, wires, creature, rudolph,\n",
      "Nearest to years: trinomial, two, pickles, mya, masking, melody, pp, dug,\n",
      "Nearest to about: dek, qxd, blondie, processors, five, formerly, birth, tripod,\n",
      "Nearest to his: their, the, s, its, judgment, operatorname, organs, this,\n",
      "Nearest to into: on, following, in, to, with, from, codes, and,\n",
      "Nearest to system: ulyanov, jet, ajmer, airshow, feldspar, rudolph, it, babylonians,\n",
      "Nearest to will: would, to, could, can, regulatory, may, angels, although,\n",
      "Nearest to to: for, will, nine, and, from, organs, imran, apatosaurus,\n",
      "Nearest to all: cardinality, transistor, many, sayings, individual, dunstan, allen, adherence,\n",
      "Nearest to for: in, with, and, of, from, by, to, khz,\n",
      "Nearest to seven: nine, eight, five, six, four, three, zero, dasyprocta,\n",
      "Nearest to no: suits, shrugged, marcellus, transcontinental, historically, dhamma, onyx, he,\n",
      "Nearest to over: with, berg, agave, metropolitan, agouti, heathen, periodicals, dhamma,\n",
      "Nearest to had: has, have, was, hermes, scarlatti, is, finally, anagrams,\n",
      "Nearest to when: and, claim, qualities, if, incidental, after, in, agouti,\n",
      "Nearest to most: reflector, weaver, tagalog, finally, brutality, waterloo, shipowner, antoninus,\n",
      "Average loss at step  32000 :  5.95147876060009\n",
      "Average loss at step  34000 :  5.684182981967926\n",
      "Average loss at step  36000 :  5.7897571026086805\n",
      "Average loss at step  38000 :  5.4846642234325405\n",
      "Average loss at step  40000 :  5.2493481369018555\n",
      "Nearest to called: brew, and, epistle, birkenau, selected, wires, rudolph, built,\n",
      "Nearest to years: trinomial, mya, pickles, four, melody, masking, five, pp,\n",
      "Nearest to about: dek, blondie, qxd, tripod, five, birth, formerly, circ,\n",
      "Nearest to his: their, the, its, s, judgment, operatorname, her, a,\n",
      "Nearest to into: from, following, on, in, with, for, between, codes,\n",
      "Nearest to system: ulyanov, ajmer, jet, feldspar, airshow, obtaining, rudolph, messy,\n",
      "Nearest to will: would, can, could, may, to, regulatory, should, angels,\n",
      "Nearest to to: nine, for, will, dressed, apatosaurus, not, with, journeys,\n",
      "Nearest to all: transistor, many, cardinality, sayings, individual, allen, dunstan, these,\n",
      "Nearest to for: with, in, of, to, from, and, by, grapevine,\n",
      "Nearest to seven: eight, six, five, four, three, nine, zero, two,\n",
      "Nearest to no: suits, shrugged, marcellus, dhamma, that, he, vdc, onyx,\n",
      "Nearest to over: agave, berg, agouti, with, metropolitan, heathen, dhamma, vh,\n",
      "Nearest to had: has, have, was, hermes, were, agouti, ep, anagrams,\n",
      "Nearest to when: if, after, qualities, claim, and, incidental, tainted, became,\n",
      "Nearest to most: reflector, weaver, brutality, tagalog, slovakian, huron, waterloo, agouti,\n",
      "Average loss at step  42000 :  5.385720333337784\n",
      "Average loss at step  44000 :  5.23621472787857\n",
      "Average loss at step  46000 :  5.215788307905197\n",
      "Average loss at step  48000 :  5.272521003842354\n",
      "Average loss at step  50000 :  4.987491705417633\n",
      "Nearest to called: and, brew, epistle, birkenau, selected, leads, built, dasyprocta,\n",
      "Nearest to years: trinomial, four, mya, pickles, melody, masking, two, anticipate,\n",
      "Nearest to about: dek, three, tripod, qxd, blondie, birth, five, circ,\n",
      "Nearest to his: their, the, its, s, her, kapoor, judgment, operatorname,\n",
      "Nearest to into: from, following, on, in, to, between, with, allows,\n",
      "Nearest to system: ulyanov, ajmer, jet, feldspar, airshow, messy, priestly, obtaining,\n",
      "Nearest to will: would, can, could, may, must, to, should, regulatory,\n",
      "Nearest to to: will, nine, would, and, apatosaurus, from, imran, dressed,\n",
      "Nearest to all: many, transistor, cardinality, sayings, these, allen, individual, dunstan,\n",
      "Nearest to for: in, with, of, khz, five, and, from, operatorname,\n",
      "Nearest to seven: eight, six, five, three, four, zero, nine, dasyprocta,\n",
      "Nearest to no: shrugged, suits, marcellus, transcontinental, vdc, dhamma, onyx, calls,\n",
      "Nearest to over: agave, berg, kapoor, agouti, four, three, heathen, metropolitan,\n",
      "Nearest to had: has, have, was, were, by, hermes, anagrams, agouti,\n",
      "Nearest to when: and, after, if, while, claim, but, incidental, alphorn,\n",
      "Nearest to most: reflector, weaver, more, brutality, huron, agouti, slovakian, rosters,\n",
      "Average loss at step  52000 :  5.045447431325912\n",
      "Average loss at step  54000 :  5.19867452454567\n",
      "Average loss at step  56000 :  5.056689159989357\n",
      "Average loss at step  58000 :  5.06572107219696\n",
      "Average loss at step  60000 :  4.951277780890464\n",
      "Nearest to called: brew, and, epistle, leads, selected, hut, birkenau, adult,\n",
      "Nearest to years: trinomial, mya, pickles, two, masking, pp, melody, alexandrovna,\n",
      "Nearest to about: dek, blondie, tripod, five, qxd, formerly, birth, six,\n",
      "Nearest to his: their, its, her, the, s, judgment, operatorname, kapoor,\n",
      "Nearest to into: from, following, on, in, allows, between, through, contemporaneous,\n",
      "Nearest to system: ulyanov, ajmer, feldspar, ursus, messy, jet, airshow, grizzly,\n",
      "Nearest to will: would, can, could, may, should, must, to, regulatory,\n",
      "Nearest to to: will, imran, for, nine, dressed, apatosaurus, journeys, would,\n",
      "Nearest to all: many, transistor, microsite, these, two, cardinality, ursus, three,\n",
      "Nearest to for: in, of, with, and, khz, alumni, or, to,\n",
      "Nearest to seven: eight, six, nine, five, zero, four, three, circ,\n",
      "Nearest to no: shrugged, marcellus, suits, any, dhamma, a, vdc, onyx,\n",
      "Nearest to over: agave, kapoor, agouti, berg, four, ursus, heathen, touching,\n",
      "Nearest to had: has, have, was, were, anagrams, agouti, dasyprocta, by,\n",
      "Nearest to when: if, after, while, but, was, although, during, where,\n",
      "Nearest to most: more, many, reflector, weaver, waterloo, rosters, agouti, slovakian,\n",
      "Average loss at step  62000 :  5.022963812351227\n",
      "Average loss at step  64000 :  4.839528342604637\n",
      "Average loss at step  66000 :  4.597828418850899\n",
      "Average loss at step  68000 :  4.962491476655006\n",
      "Average loss at step  70000 :  4.8854131294488905\n",
      "Nearest to called: brew, epistle, leads, and, UNK, selected, birkenau, hut,\n",
      "Nearest to years: trinomial, mya, pickles, speleology, alexandrovna, anticipate, masking, fcc,\n",
      "Nearest to about: dek, blondie, tripod, four, formerly, three, birth, qxd,\n",
      "Nearest to his: their, its, the, her, s, callithrix, kapoor, operatorname,\n",
      "Nearest to into: from, following, through, on, allows, between, in, with,\n",
      "Nearest to system: messy, ajmer, feldspar, ursus, ulyanov, obtaining, rudolph, fennel,\n",
      "Nearest to will: would, can, could, may, must, should, to, regulatory,\n",
      "Nearest to to: will, can, microcebus, imran, apatosaurus, would, dressed, histories,\n",
      "Nearest to all: many, these, transistor, microsite, ursus, some, sayings, cardinality,\n",
      "Nearest to for: of, prat, and, with, khz, microcebus, in, alumni,\n",
      "Nearest to seven: eight, six, nine, five, four, three, zero, dasyprocta,\n",
      "Nearest to no: shrugged, marcellus, saguinus, any, infectors, suits, a, vdc,\n",
      "Nearest to over: agave, berg, kapoor, agouti, four, heathen, three, ursus,\n",
      "Nearest to had: has, have, was, were, by, been, anagrams, agouti,\n",
      "Nearest to when: if, while, after, but, where, and, during, although,\n",
      "Nearest to most: more, many, reflector, rosters, waterloo, weaver, agouti, slovakian,\n",
      "Average loss at step  72000 :  4.735564237117767\n",
      "Average loss at step  74000 :  4.803279079437256\n",
      "Average loss at step  76000 :  4.736880506157875\n",
      "Average loss at step  78000 :  4.802402430653572\n",
      "Average loss at step  80000 :  4.805684024631977\n",
      "Nearest to called: brew, epistle, and, selected, leads, dasyprocta, built, circ,\n",
      "Nearest to years: trinomial, mya, pickles, speleology, alexandrovna, anticipate, fcc, masking,\n",
      "Nearest to about: dek, tripod, blondie, birth, catolica, formerly, upanija, agp,\n",
      "Nearest to his: their, its, her, the, s, busan, callithrix, kapoor,\n",
      "Nearest to into: from, through, allows, following, with, between, in, to,\n",
      "Nearest to system: feldspar, messy, ulyanov, ajmer, ursus, systems, grizzly, fennel,\n",
      "Nearest to will: would, can, could, may, must, should, to, positivist,\n",
      "Nearest to to: microcebus, imran, nine, apatosaurus, can, unassigned, will, circ,\n",
      "Nearest to all: many, some, these, ursus, both, transistor, microsite, sayings,\n",
      "Nearest to for: of, in, and, thaler, or, prat, microcebus, with,\n",
      "Nearest to seven: six, eight, five, four, nine, three, zero, two,\n",
      "Nearest to no: marcellus, shrugged, saguinus, any, infectors, johansen, vdc, dhamma,\n",
      "Nearest to over: kapoor, agave, agouti, berg, ursus, heathen, thaler, touching,\n",
      "Nearest to had: has, have, was, were, by, anagrams, been, agouti,\n",
      "Nearest to when: if, while, after, where, but, was, during, although,\n",
      "Nearest to most: more, many, rosters, some, agouti, slovakian, waterloo, reflector,\n",
      "Average loss at step  82000 :  4.761036015748978\n",
      "Average loss at step  84000 :  4.743689119458199\n",
      "Average loss at step  86000 :  4.774340403676033\n",
      "Average loss at step  88000 :  4.74596404838562\n",
      "Average loss at step  90000 :  4.729918384552002\n",
      "Nearest to called: brew, and, epistle, leads, selected, dasyprocta, abet, circ,\n",
      "Nearest to years: trinomial, mya, pickles, speleology, alexandrovna, anticipate, fcc, masking,\n",
      "Nearest to about: dek, tripod, blondie, globemaster, catolica, formerly, upanija, agp,\n",
      "Nearest to his: their, its, her, the, s, callithrix, judgment, kapoor,\n",
      "Nearest to into: from, through, following, allows, between, with, in, to,\n",
      "Nearest to system: systems, ulyanov, ajmer, ursus, messy, feldspar, grizzly, rudolph,\n",
      "Nearest to will: would, can, may, could, must, should, to, might,\n",
      "Nearest to to: microcebus, will, can, nine, would, imran, unassigned, apatosaurus,\n",
      "Nearest to all: many, both, some, these, ursus, microsite, transistor, individual,\n",
      "Nearest to for: prat, with, in, thaler, khz, of, alumni, to,\n",
      "Nearest to seven: eight, five, six, four, nine, three, zero, callithrix,\n",
      "Nearest to no: any, shrugged, marcellus, saguinus, vdc, it, onyx, infectors,\n",
      "Nearest to over: berg, kapoor, agouti, agave, ursus, heathen, thaler, touching,\n",
      "Nearest to had: has, have, was, were, anagrams, escuela, by, dasyprocta,\n",
      "Nearest to when: if, after, while, where, during, but, although, before,\n",
      "Nearest to most: more, many, some, rosters, slovakian, agouti, waterloo, reflector,\n",
      "Average loss at step  92000 :  4.674594023585319\n",
      "Average loss at step  94000 :  4.726039476871491\n",
      "Average loss at step  96000 :  4.685625111818314\n",
      "Average loss at step  98000 :  4.601665502429008\n",
      "Average loss at step  100000 :  4.695355836868286\n",
      "Nearest to called: brew, and, abies, leads, selected, epistle, cuauht, archived,\n",
      "Nearest to years: trinomial, pickles, mya, alexandrovna, anticipate, two, speleology, fcc,\n",
      "Nearest to about: dek, tripod, blondie, globemaster, agp, four, over, clockless,\n",
      "Nearest to his: their, her, its, the, s, callithrix, kapoor, operatorname,\n",
      "Nearest to into: through, from, following, allows, with, between, towards, marek,\n",
      "Nearest to system: systems, ajmer, feldspar, ulyanov, messy, ursus, orbit, cv,\n",
      "Nearest to will: would, can, may, could, must, should, might, to,\n",
      "Nearest to to: microcebus, imran, can, will, would, unassigned, journeys, circ,\n",
      "Nearest to all: many, some, both, these, microsite, ursus, several, transistor,\n",
      "Nearest to for: thaler, in, of, prat, operatorname, with, microcebus, eight,\n",
      "Nearest to seven: eight, six, five, four, nine, three, zero, two,\n",
      "Nearest to no: any, shrugged, marcellus, saguinus, dhamma, vdc, johansen, onyx,\n",
      "Nearest to over: kapoor, agave, agouti, berg, heathen, touching, only, ursus,\n",
      "Nearest to had: has, have, was, were, anagrams, escuela, agouti, dasyprocta,\n",
      "Nearest to when: if, after, while, where, before, during, although, but,\n",
      "Nearest to most: more, many, some, rosters, among, agouti, slovakian, use,\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "\n",
    "def word2vec_basic(log_dir):\n",
    "  \"\"\"Example of building, training and visualizing a word2vec model.\"\"\"\n",
    "  # Create the directory for TensorBoard variables if there is not.\n",
    "  if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "  # Step 1: Download the data.\n",
    "  url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "  # pylint: disable=redefined-outer-name\n",
    "  def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    local_filename = os.path.join(gettempdir(), filename)\n",
    "    if not os.path.exists(local_filename):\n",
    "      local_filename, _ = urllib.request.urlretrieve(url + filename,\n",
    "                                                     local_filename)\n",
    "    statinfo = os.stat(local_filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "      print('Found and verified', filename)\n",
    "    else:\n",
    "      print(statinfo.st_size)\n",
    "      raise Exception('Failed to verify ' + local_filename +\n",
    "                      '. Can you get to it with a browser?')\n",
    "    return local_filename\n",
    "\n",
    "  filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "  # Read the data into a list of strings.\n",
    "  def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "      data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "  vocabulary = read_data(filename)\n",
    "  print('Data size', len(vocabulary))\n",
    "\n",
    "  # Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "  vocabulary_size = 50000\n",
    "\n",
    "  def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "      dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "      index = dictionary.get(word, 0)\n",
    "      if index == 0:  # dictionary['UNK']\n",
    "        unk_count += 1\n",
    "      data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "  # Filling 4 global variables:\n",
    "  # data - list of codes (integers from 0 to vocabulary_size-1).\n",
    "  #   This is the original text but words are replaced by their codes\n",
    "  # count - map of words(strings) to count of occurrences\n",
    "  # dictionary - map of words(strings) to their codes(integers)\n",
    "  # reverse_dictionary - maps codes(integers) to words(strings)\n",
    "  data, count, unused_dictionary, reverse_dictionary = build_dataset(\n",
    "      vocabulary, vocabulary_size)\n",
    "  del vocabulary  # Hint to reduce memory.\n",
    "  print('Most common words (+UNK)', count[:5])\n",
    "  print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "  # Step 3: Function to generate a training batch for the skip-gram model.\n",
    "  def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builtin\n",
    "    if data_index + span > len(data):\n",
    "      data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "      context_words = [w for w in range(span) if w != skip_window]\n",
    "      words_to_use = random.sample(context_words, num_skips)\n",
    "      for j, context_word in enumerate(words_to_use):\n",
    "        batch[i * num_skips + j] = buffer[skip_window]\n",
    "        labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "      if data_index == len(data):\n",
    "        buffer.extend(data[0:span])\n",
    "        data_index = span\n",
    "      else:\n",
    "        buffer.append(data[data_index])\n",
    "        data_index += 1\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "  batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "  for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n",
    "          reverse_dictionary[labels[i, 0]])\n",
    "  # Step 4: Build and train a skip-gram model.\n",
    "\n",
    "  batch_size = 128\n",
    "  embedding_size = 128  # Dimension of the embedding vector.\n",
    "  skip_window = 1  # How many words to consider left and right.\n",
    "  num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "  num_sampled = 64  # Number of negative examples to sample.\n",
    "\n",
    "  # We pick a random validation set to sample nearest neighbors. Here we limit\n",
    "  # the validation samples to the words that have a low numeric ID, which by\n",
    "  # construction are also the most frequent. These 3 variables are used only for\n",
    "  # displaying model accuracy, they don't affect calculation.\n",
    "  valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "  valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "  valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "  graph = tf.Graph()\n",
    "\n",
    "  with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    with tf.name_scope('inputs'):\n",
    "      train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "      train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "      valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "      # Look up embeddings for inputs.\n",
    "      with tf.name_scope('embeddings'):\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "      # Construct the variables for the NCE loss\n",
    "      with tf.name_scope('weights'):\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "      with tf.name_scope('biases'):\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    with tf.name_scope('loss'):\n",
    "      loss = tf.reduce_mean(\n",
    "          tf.nn.nce_loss(\n",
    "              weights=nce_weights,\n",
    "              biases=nce_biases,\n",
    "              labels=train_labels,\n",
    "              inputs=embed,\n",
    "              num_sampled=num_sampled,\n",
    "              num_classes=vocabulary_size))\n",
    "\n",
    "    # Add the loss value as a scalar to summary.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    with tf.name_scope('optimizer'):\n",
    "      optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all\n",
    "    # embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "                                              valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Merge all summaries.\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "  # Step 5: Begin training.\n",
    "  num_steps = 100001\n",
    "\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    # Open a writer to write summaries.\n",
    "    writer = tf.summary.FileWriter(log_dir, session.graph)\n",
    "\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "      batch_inputs, batch_labels = generate_batch(batch_size, num_skips,\n",
    "                                                  skip_window)\n",
    "      feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "      # Define metadata variable.\n",
    "      run_metadata = tf.RunMetadata()\n",
    "\n",
    "      # We perform one update step by evaluating the optimizer op (including it\n",
    "      # in the list of returned values for session.run()\n",
    "      # Also, evaluate the merged op to get all summaries from the returned\n",
    "      # \"summary\" variable. Feed metadata variable to session for visualizing\n",
    "      # the graph in TensorBoard.\n",
    "      _, summary, loss_val = session.run([optimizer, merged, loss],\n",
    "                                         feed_dict=feed_dict,\n",
    "                                         run_metadata=run_metadata)\n",
    "      average_loss += loss_val\n",
    "\n",
    "      # Add returned summaries to writer in each step.\n",
    "      writer.add_summary(summary, step)\n",
    "      # Add metadata to visualize the graph for the last run.\n",
    "      if step == (num_steps - 1):\n",
    "        writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "      if step % 2000 == 0:\n",
    "        if step > 0:\n",
    "          average_loss /= 2000\n",
    "        # The average loss is an estimate of the loss over the last 2000\n",
    "        # batches.\n",
    "        print('Average loss at step ', step, ': ', average_loss)\n",
    "        average_loss = 0\n",
    "\n",
    "      # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "      if step % 10000 == 0:\n",
    "        sim = similarity.eval()\n",
    "        for i in xrange(valid_size):\n",
    "          valid_word = reverse_dictionary[valid_examples[i]]\n",
    "          top_k = 8  # number of nearest neighbors\n",
    "          nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "          log_str = 'Nearest to %s:' % valid_word\n",
    "          for k in xrange(top_k):\n",
    "            close_word = reverse_dictionary[nearest[k]]\n",
    "            log_str = '%s %s,' % (log_str, close_word)\n",
    "          print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    # Write corresponding labels for the embeddings.\n",
    "    with open(log_dir + '/metadata.tsv', 'w') as f:\n",
    "      for i in xrange(vocabulary_size):\n",
    "        f.write(reverse_dictionary[i] + '\\n')\n",
    "\n",
    "    # Save the model for checkpoints.\n",
    "    saver.save(session, os.path.join(log_dir, 'model.ckpt'))\n",
    "\n",
    "    # Create a configuration for visualizing embeddings with the labels in\n",
    "    # TensorBoard.\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = embeddings.name\n",
    "    embedding_conf.metadata_path = os.path.join(log_dir, 'metadata.tsv')\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "\n",
    "  writer.close()\n",
    "\n",
    "  # Step 6: Visualize the embeddings.\n",
    "\n",
    "  # pylint: disable=missing-docstring\n",
    "  # Function to draw visualization of distance between embeddings.\n",
    "  def plot_with_labels(low_dim_embs, labels, filename):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "      x, y = low_dim_embs[i, :]\n",
    "      plt.scatter(x, y)\n",
    "      plt.annotate(\n",
    "          label,\n",
    "          xy=(x, y),\n",
    "          xytext=(5, 2),\n",
    "          textcoords='offset points',\n",
    "          ha='right',\n",
    "          va='bottom')\n",
    "\n",
    "    plt.savefig(filename)\n",
    "\n",
    "  try:\n",
    "    # pylint: disable=g-import-not-at-top\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    tsne = TSNE(\n",
    "        perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "    plot_only = 500\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "    plot_with_labels(low_dim_embs, labels, os.path.join(gettempdir(),\n",
    "                                                        'tsne.png'))\n",
    "\n",
    "  except ImportError as ex:\n",
    "    print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "    print(ex)\n",
    "\n",
    "\n",
    "# All functionality is run after tf.app.run() (b/122547914). This could be split\n",
    "# up but the methods are laid sequentially with their usage for clarity.\n",
    "def main(unused_argv):\n",
    "  # Give a folder path as an argument with '--log_dir' to save\n",
    "  # TensorBoard summaries. Default is a log folder in current directory.\n",
    "  current_path = os.path.dirname(os.path.realpath(sys.argv[0]))\n",
    "\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--log_dir',\n",
    "      type=str,\n",
    "      default=os.path.join(current_path, 'log'),\n",
    "      help='The log directory for TensorBoard summaries.')\n",
    "  flags, unused_flags = parser.parse_known_args()\n",
    "  word2vec_basic(flags.log_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
